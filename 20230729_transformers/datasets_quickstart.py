"""
Datasets æ˜¯ä¸€ä¸ªåº“ï¼Œç”¨äºè½»æ¾è®¿é—®å’Œåˆ†äº«éŸ³é¢‘ã€è®¡ç®—æœºè§†è§‰å’Œè‡ª
ç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡çš„æ•°æ®é›†ã€‚

åªéœ€ä¸€è¡Œä»£ç å³å¯åŠ è½½æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨æˆ‘ä»¬å¼ºå¤§çš„æ•°æ®å¤„ç†æ–¹æ³•å¿«
é€Ÿä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹å‡†å¤‡æ•°æ®é›†ã€‚ç”± Apache Arrow æ ¼å¼æ”¯æŒï¼Œå¯
ä»¥åœ¨æ²¡æœ‰ä»»ä½•å†…å­˜é™åˆ¶çš„æƒ…å†µä¸‹å¤„ç†å¤§å‹æ•°æ®é›†ï¼Œå®ç°é›¶å¤åˆ¶è¯»å–ï¼Œ
ä»¥è·å¾—æœ€ä½³çš„é€Ÿåº¦å’Œæ•ˆç‡ã€‚æˆ‘ä»¬è¿˜ä¸ Hugging Face Hub æ·±åº¦é›†
æˆï¼Œä½¿æ‚¨å¯ä»¥è½»æ¾åŠ è½½å’Œä¸æ›´å¹¿æ³›çš„æœºå™¨å­¦ä¹ ç¤¾åŒºåˆ†äº«æ•°æ®é›†ã€‚

ä»Šå¤©åœ¨ Hugging Face Hub ä¸Šæ‰¾åˆ°æ‚¨çš„æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨å®æ—¶æŸ¥çœ‹å™¨
æ·±å…¥æŸ¥çœ‹å®ƒçš„å†…éƒ¨ã€‚
"""


from datasets import load_dataset
from datasets import get_dataset_split_names
from dotenv import load_dotenv, find_dotenv
from datasets import load_dataset_builder

_ = load_dotenv(find_dotenv())

"""
åŠ è½½æ•°æ®é›†
åœ¨æ‚¨èŠ±æ—¶é—´ä¸‹è½½æ•°æ®é›†ä¹‹å‰ï¼Œå¿«é€Ÿè·å–æœ‰å…³æ•°æ®é›†çš„ä¸€äº›å¸¸è§„ä¿¡æ¯é€šå¸¸
å¾ˆæœ‰å¸®åŠ©ã€‚æ•°æ®é›†çš„ä¿¡æ¯å­˜å‚¨åœ¨ DatasetInfo ä¸­ï¼Œå¯ä»¥åŒ…æ‹¬æ•°æ®é›†æ
è¿°ã€ç‰¹å¾å’Œæ•°æ®é›†å¤§å°ç­‰ä¿¡æ¯ã€‚

ä½¿ç”¨ load_dataset_builder() å‡½æ•°åŠ è½½æ•°æ®é›†æ„å»ºå™¨ï¼Œå¹¶åœ¨ä¸ä¸‹è½½
çš„æƒ…å†µä¸‹æ£€æŸ¥æ•°æ®é›†çš„å±æ€§ã€‚
"""
ds_builder = load_dataset_builder("rotten_tomatoes")


print(ds_builder.info.description)

print(ds_builder.info.features)

"""
åˆ‡åˆ†
åˆ‡åˆ†æ˜¯æ•°æ®é›†çš„ç‰¹å®šå­é›†ï¼Œä¾‹å¦‚è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
ä½¿ç”¨ get_dataset_split_names() å‡½æ•°åˆ—å‡º
æ•°æ®é›†çš„åˆ‡åˆ†åç§°ã€‚

ä¸€èˆ¬åˆ†ä¸ºï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
"""
get_dataset_split_names("rotten_tomatoes") # ['train', 'validation', 'test']


"""
ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ split å‚æ•°åŠ è½½ç‰¹å®šçš„åˆ‡åˆ†ã€‚åŠ è½½æ•°æ®é›†åˆ‡åˆ†å°†è¿”å›ä¸€ä¸ª Dataset å¯¹è±¡ã€‚

æ¯”å¦‚ split="train" å‚æ•°ç”¨äºåŠ è½½è®­ç»ƒé›†
"""
dataset = load_dataset("rotten_tomatoes", split="train")
print(dataset)


"""
å¦‚æœæ‚¨æ²¡æœ‰æŒ‡å®šä¸€ä¸ªåˆ‡åˆ†ï¼ŒğŸ¤— Datasets ä¼šè¿”å›ä¸€ä¸ª DatasetDict å¯¹è±¡ã€‚

DatasetDict çš„æè¿°ä¿¡æ¯å¦‚ä¸‹ï¼Œæè¿°äº†æ•°æ®é›†çš„åç§°ã€ç‰¹å¾ã€æ•°æ®æ¡æ•°ï¼š

DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 8530
    })
    validation: Dataset({
        features: ['text', 'label'],
        num_rows: 1066
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 1066
    })
})

"""
dataset = load_dataset("rotten_tomatoes")
print(dataset)

